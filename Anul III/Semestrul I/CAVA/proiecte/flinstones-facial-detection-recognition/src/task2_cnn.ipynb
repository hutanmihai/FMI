{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c7929bf3dad5d28"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "\n",
    "from src.constants import (\n",
    "    COLLAPSED_ANNOTATIONS_PATH,\n",
    "    POSITIVES_PATH,\n",
    "    POSITIVES_VALIDATION_PATH,\n",
    "    VALIDATION_ANNOTATIONS_PATH,\n",
    "    MODEL_PATH,\n",
    "    LABELS_MAP,\n",
    ")\n",
    "from src.utils.readers import get_images, get_annotations\n",
    "\n",
    "from src.utils.helpers import check_if_dirs_exist"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77f133cac5222642",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac246d817886e2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"../\" + str(MODEL_PATH))\n",
    "POSITIVES_PATH = Path(\"../\" + str(POSITIVES_PATH))\n",
    "POSITIVES_VALIDATION_PATH = Path(\"../\" + str(POSITIVES_VALIDATION_PATH))\n",
    "COLLAPSED_ANNOTATIONS_PATH = Path(\"../\" + str(COLLAPSED_ANNOTATIONS_PATH))\n",
    "VALIDATION_ANNOTATIONS_PATH = Path(\"../\" + str(VALIDATION_ANNOTATIONS_PATH))\n",
    "\n",
    "INIT_LR = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83a319fca0f04fe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initializations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ef9fcd2b54222a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0a884308170696e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data loading"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea4ee06f644be9d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(LABELS_MAP[label], dtype=torch.long)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7284c4872dbf6103",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainImages = get_images(POSITIVES_PATH)\n",
    "trainImages = [cv.cvtColor(image, cv.COLOR_BGR2RGB) for image in trainImages]\n",
    "trainAnnotations = get_annotations(COLLAPSED_ANNOTATIONS_PATH)\n",
    "trainLabels = []\n",
    "\n",
    "valImages = get_images(POSITIVES_VALIDATION_PATH)\n",
    "valImages = [cv.cvtColor(image, cv.COLOR_BGR2RGB) for image in valImages]\n",
    "valAnnotations = get_annotations(VALIDATION_ANNOTATIONS_PATH)\n",
    "valLabels = []\n",
    "\n",
    "for image_name, detections_list in trainAnnotations.items():\n",
    "    for _, char in detections_list:\n",
    "        trainLabels.append(char)\n",
    "        trainLabels.append(char)\n",
    "\n",
    "for image_name, detections_list in valAnnotations.items():\n",
    "    for _, char in detections_list:\n",
    "        valLabels.append(char)\n",
    "        valLabels.append(char)\n",
    "\n",
    "trainDataset = CustomDataset(trainImages, trainLabels)\n",
    "valDataset = CustomDataset(valImages, valLabels)\n",
    "\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valDataLoader = DataLoader(valDataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"[INFO] number of training images: {}\".format(len(trainImages)))\n",
    "print(\"[INFO] number of training labels: {}\".format(len(trainLabels)))\n",
    "print(\"[INFO] number of validation images: {}\".format(len(valImages)))\n",
    "print(\"[INFO] number of validation labels: {}\".format(len(valLabels)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfca3f47432ccdf7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d26c7ebdcaaa96"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "found = set()  # Using a set to store unique labels found\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))  # Creating subplots for 5 images\n",
    "\n",
    "for images, labels in trainDataLoader:\n",
    "    for image, label in zip(images, labels):\n",
    "        if label.item() not in found:\n",
    "            found.add(label.item())\n",
    "            image = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "            # Plot the image in the next available subplot\n",
    "            ax = axes[len(found) - 1]\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(label)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        if len(found) == 5:\n",
    "            break\n",
    "    if len(found) == 5:\n",
    "        break\n",
    "\n",
    "# Show the subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35e2d469d196c7dd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data frequency"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db29b6089988a7b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Choose between trainLabels or valLabels\n",
    "labels = trainLabels  # Change this to valLabels for validation dataset\n",
    "\n",
    "# Count occurrences of each label\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.title(\"Label Frequency Distribution\")\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e09b2ae0e82e27b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd461faabde96911"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    # Input: 3x40x40\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(256, 5),\n",
    "    nn.Softmax(1),\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26a5e5a7184664c9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizer and loss function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55ce6a99c08c854b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=INIT_LR)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
    "valSteps = len(valDataLoader.dataset) // BATCH_SIZE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53d85cd8fcd520e4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "499c9c72432142e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    model.train()\n",
    "    totalTrainLoss = 0\n",
    "    trainCorrect = 0\n",
    "\n",
    "    for x, y in trainDataLoader:\n",
    "        optimizer.zero_grad()\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        loss = loss_function(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totalTrainLoss += loss.item()\n",
    "\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        trainCorrect += (pred == y).sum().item()\n",
    "\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    trainAccuracy = trainCorrect / len(trainDataLoader.dataset)\n",
    "\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(avgTrainLoss, trainAccuracy))\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dfa94eef745b540",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test the model on validation data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd4cc3c9722fb14"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.eval()\n",
    "valCorrect = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in valDataLoader:\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        pred = model(x)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        valCorrect += (pred == y).sum().item()\n",
    "\n",
    "valAccuracy = valCorrect / len(valDataLoader.dataset)\n",
    "\n",
    "print(\"Validation accuracy: {:.4f}\".format(valAccuracy))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d0ac8fcc0caa19b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Saving the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee763d0e0fd398c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "check_if_dirs_exist([MODEL_PATH])\n",
    "model.save(str(MODEL_PATH / \"task2_cnn.pth\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d7f4e2347cb26c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
