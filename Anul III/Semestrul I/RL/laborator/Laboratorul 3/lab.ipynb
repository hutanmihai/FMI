{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# #uncomment only if you're running from google colab\n",
    "# !ls\n",
    "# !pip install gym #For full installations, see https://github.com/openai/gym#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#from TaxiAgents import TrainTaxiAgent, TaxiAgent, SARSA_TaxiAgent, QLearning_TaxiAgent, ExpectedSARSA_TaxiAgent\n",
    "from TaxiAgents_sol import TrainTaxiAgent, TaxiAgent, SARSA_TaxiAgent, QLearning_TaxiAgent, ExpectedSARSA_TaxiAgent\n",
    "\n",
    "\n",
    "MODEL_PATH = 'models/'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal difference (TD) learning is another value-based method for solving a reinforcement learning problem. Main differences between this and the Monte Carlo method we learned are that:\n",
    "\n",
    "* TD updates the Q-values at every time step; we do not need to run an entire episode beforehand\n",
    "* TD updates the policy at every timestep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try [Taxi-v3](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taxi-v3 is a reinforcment learning environment described in [Dietterich (2000)](https://arxiv.org/abs/cs/9905014) to demonstrate some issues with hierarchical reinforcement learning. The environment features a 5x5 grid with four locations denoted as red, blue, green and yellow. Our agent is a taxi started at any random position. The goal of the environment is to go pick up a passenger at an arbitrary location then send them to their destination as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3', render_mode='ansi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out what are the following elements\n",
    "\n",
    "* States:\n",
    "* Actions:\n",
    "* Rewards: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use this cell to try execute the following functions:\n",
    "env.reset() - reset the environment and return the current state\n",
    "env.step(action) - take an action at the current state; returns a tuple of (state, reward, done, info)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space (State Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is a single integer from 0 to 499. This is a result of all combinations of:\n",
    "* 25 taxi positions\n",
    "* 5 possible locations of the passenger (including the case when the passenger is the taxi)\n",
    "* 4 destination locations. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* -1 for each action; to finish the task as fast as possible\n",
    "* +20 for delivering passenger\n",
    "* -10 for doing `pickup` (4) and `dropoff` (5) illegally\n",
    "* No penalty for illegal move (the taxi will remain at the same tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'State: {env.reset()}')\n",
    "next_tuple = env.step(1)\n",
    "print(f'Action: 0, Next state: {next_tuple[0]}, Reward: {next_tuple[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters: locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(env.render(mode = 'ansi'))\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Play - check your understanding by playing the game !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a couple of rounds with your own natural neural networks to get a hang of the environment.\n",
    "Do not forget that:\n",
    "* blue colored is the location of the passenger  (or green taxi if he is in taxi)\n",
    "* mangeta color is the destination\n",
    "* you can take a look at their github implementation inside functions encode and decode if unclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this if it's too annoying in your request to run all cells\n",
    "play = True\n",
    "if play:\n",
    "    state = env.reset()\n",
    "    print(env.render())\n",
    "\n",
    "    cumu_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = int(input('Choose action: '))\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        cumu_reward+=reward\n",
    "        clear_output(wait=False)\n",
    "        print(env.render())\n",
    "        print(f'State: {state}')\n",
    "        print(f'Reward: {reward}')\n",
    "        print(f'Cumulative Reward: {cumu_reward}')\n",
    "        if reward==20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0: Down\n",
    "- 1: Up\n",
    "- 2: Right\n",
    "- 3: Left\n",
    "- 4: Pickup\n",
    "- 5: Dropoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try the random / default TaxiAgent in file TaxiAgents.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a function that simulates a play and provide statistics using a given TaxiAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# These are helper function to print the frames graphically using a given framerate, no need to read it !\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# And this is for simulation of an environment, we'll use it later\n",
    "def simulateEnvironment(env, taxiAgent):\n",
    "    frames = []\n",
    "    illegal_moves = 0\n",
    "    illegal_others = 0\n",
    "    cumu_reward = 0\n",
    "\n",
    "    #get initial state\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    while True:\n",
    "        #best action\n",
    "        action = taxiAgent.select_action(state, taxiAgent.get_epsilon(1e6))\n",
    "\n",
    "        #step the environment to get (next_state, reward, done, info)\n",
    "        next_state,reward,done,info,_ =  env.step(action)\n",
    "\n",
    "        #keep records\n",
    "        if state==next_state: illegal_moves+=1\n",
    "        if reward==-10: illegal_others+=1\n",
    "        cumu_reward+=reward\n",
    "        frames.append({\n",
    "            'frame': env.render(),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'cumulative_reward': cumu_reward,\n",
    "            'illegal_moves': illegal_moves,\n",
    "            'illegal_others': illegal_others\n",
    "            })\n",
    "\n",
    "        #set current state to next state\n",
    "        state = next_state\n",
    "        \n",
    "        #print(f\"taking action:{action}, next_state is {state}\")\n",
    "\n",
    "        #stop game if done; there's a bug with taxi-v2 that cuts your game over if you have not solved it in 200 timesteps\n",
    "        if reward==20: break\n",
    "            \n",
    "    print_frames(frames)\n",
    "\n",
    "def print_frames(frames,wait_time=0.2):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        print(f\"Cumulative Reward: {frame['cumulative_reward']}\")\n",
    "        print(f\"Illegal Moves: {frame['illegal_moves']}\")\n",
    "        print(f\"Illegal Others: {frame['illegal_others']}\")\n",
    "        time.sleep(wait_time)\n",
    "              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our benchmark will be the final cumulative reward, from now on called `score`, at the end of an episode over time. For instance, when we do 1,000 episodes, as our agent becomes smarter, the cumulative rewards should get better and better. OpenAI uses 100-episode average score as their benchmark for \"solving\" the environment. **The solving score for this environment is 8.5.**\n",
    "\n",
    "\n",
    "**IMPORTANT QUESTION:** Why is it a bad idea to use Monte Carlo method for this environment?\n",
    "\n",
    "**IMPORTANT** Study they code for function TrainTaxiAgent to see the whole training loop, inside TaxiAgents.py file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement SARSA in TaxiAgents.py, class SARSA_TaxiAgent.py. \n",
    "### You will have to implement only the updateExperience function, but make sure you understand rest of the code and parent class as well\n",
    "### Then let it play below to see that it is really working\n",
    "\n",
    "Check the course again to remember the details.\n",
    "SARSA is the most basic of temporal difference learning. It got its name from the tuples we used for the learning:\n",
    "\n",
    "$$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow ... \\rightarrow S_{t-1} \\rightarrow A_{t-1} \\rightarrow R_t \\rightarrow S_{t}$$\n",
    "\n",
    "And since action value $Q(s,a)$ is **expected** discounted rewards, we can substitute $Q(s_{t+1},a_{t+1})$ for $G_{t+1}$. And with each time step, we update the action values as:\n",
    "\n",
    "$$Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t))$$\n",
    "\n",
    "where:\n",
    "* $\\alpha$ is the learning rate\n",
    "* $Q(s_t,a_t)$ is the action value for that state-action pair\n",
    "\n",
    "For every **timestep**\n",
    "* **Step 1** Given state $s_t$, choose an action $a_t$ using epsilon greedy policy\n",
    "* **Step 2** Take that action and observe next state ($s_{t+1}$) and reward ($r_{t+1}$)\n",
    "* **Step 3** Update the action value $Q(s_t,a_t)$ using the equation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a SARSA playing agent and simulate a play rendering\n",
    "sarsaAgent = SARSA_TaxiAgent(env)\n",
    "lastEpisodeFrames = TrainTaxiAgent(env, sarsaAgent, numEpisodes=100, plotStats=True, storeFramesDescription=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sarsaAgent.q)\n",
    "\n",
    "# Then check your first smart agent how it deals on plain simulation\n",
    "# Run it multiple times !!\n",
    "simulateEnvironment(env, sarsaAgent)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement Q-learning (also known as SARSAMAX) in TaxiAgents.py, class QLearning_TaxiAgent.py. \n",
    "### You will have to implement only the updateExperience function, but make sure you understand rest of the code and parent class as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning or SARSAMAX is perhaps the most well-known RL algorithm due to Deepminds' [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) paper in 2013. There are some significant contributions made by that paper such as using deep learning as function approximation instead of a dictionary, experience replay, and fixed target for learning. But the principle of the learning remains largely the same as the algorithm we will learn today.\n",
    "\n",
    "For every **timestep**\n",
    "* **Step 1** Given state $s_t$, choose an action $a_t$ using epsilon greedy policy\n",
    "* **Step 2** Update the action value $Q(s_t,a_t)$ using the following equation:\n",
    "\n",
    "$Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma max_{a}Q(s_{t+1},a) - Q(s_t,a_t))$\n",
    "\n",
    "where $max_{a}Q(s_{t+1},a)$ is the highest action value given state $s_{t+1}$. \n",
    "\n",
    "This is very similar to SARSA except for the fact that we will always choose the best action-value as opposed to epsilon-greedy action selection of SARSA. For this reason, SARSA is called **on-policy** and Q-learning **off-policy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a QLearning playing agent and simulate a play rendering\n",
    "qlearningAgent = QLearning_TaxiAgent(env)\n",
    "lastEpisodeFrames = TrainTaxiAgent(env, qlearningAgent, numEpisodes=100000, plotStats=True, storeFramesDescription=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then check your secomd smart agent how it deals on plain simulation\n",
    "# Run it multiple times !!\n",
    "simulateEnvironment(env, qlearningAgent)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implement Expected SARSA  in TaxiAgents.py, class ExpectedSARSA_TaxiAgent.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected SARSA is another way we can update the Q values, not with maximum action values but with all the actions value weighted by their probability (in our case of the epsilon-greedy policy).\n",
    "\n",
    "For every **timestep**\n",
    "* **Step 1** Given state $s_t$, choose an action $a_t$ using epsilon greedy policy\n",
    "* **Step 2** Update the action value $Q(s_t,a_t)$ using the following equation:\n",
    "\n",
    "$$Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (R_{t+1} + \\gamma \\sum_{a}\\pi(a|s_{t+1})Q(s_{t+1},a) - Q(s_t,a_t))$$\n",
    "\n",
    "where $\\pi(a|s_{t+1})$ is the probability of taking an action $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a RANDOM playing agent and simulate a play rendering\n",
    "expectedSarsaAgent = ExpectedSARSA_TaxiAgent(env)\n",
    "lastEpisodeFrames = TrainTaxiAgent(env, expectedSarsaAgent, numEpisodes=100000, plotStats=True, storeFramesDescription=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then check your third smart agent how it deals on plain simulation\n",
    "# Run it multiple times !!\n",
    "simulateEnvironment(env, expectedSarsaAgent)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI scoreboard lists 9.7 as the high score. This requires you to do some hyperparameter tuning over time of gamma, alpha and epsilon. Try to implement `get_alpha` and `get_gamma` methods in addition to `get_epsilon` and increase/decrease your hyperparameters as the training goes on. Here are some hints:\n",
    "\n",
    "* `gamma`: you would want to focus more on near-term rewards as time goes on as your agent is more aware of the action values of the states\n",
    "* `alpha`: you would want to learn more slowly as time goes on as your agent has already accumulated a lot of knowledge about the environment\n",
    "* `epsilon`: you would want to do less random action as time goes on as your best actions are probably really best.\n",
    "\n",
    "<img src=\"img/taxi_scores.PNG\" alt=\"OpenAI Learderboard\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
