{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "### If you want to check the solution directly, just uncomment the line:\n",
    "from BJAgent_sol import BJAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#cross check with our solutions once you finish\n",
    "#from BJAgent_your import BJAgent, MonteCarlo_BJAgent\n",
    "from BJAgent_sol import BJAgent, MonteCarlo_BJAgent\n",
    "\n",
    "from utils import *\n",
    "\n",
    "MODEL_PATH = 'models/'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Monte Carlo Method in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of it as a fancy way of saying **trial-and-error for some iterations then find an average**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find pi using MC:\n",
    "# - draw N samples inside a [1,1] box and if they fall inside unit circle we count them as inside.\n",
    "# The full explanation here: https://academo.org/demos/estimating-pi-monte-carlo/\n",
    "inside = 0\n",
    "n = 10**6 # 10^6\n",
    "\n",
    "x_inside = []\n",
    "y_inside = []\n",
    "x_outside = []\n",
    "y_outside = []\n",
    "\n",
    "for _ in range(n):\n",
    "    x = random()\n",
    "    y = random()\n",
    "    if x**2+y**2 <= 1:\n",
    "        inside += 1\n",
    "        x_inside.append(x)\n",
    "        y_inside.append(y)\n",
    "    else:\n",
    "        x_outside.append(x)\n",
    "        y_outside.append(y)\n",
    "\n",
    "pi = 4*inside/n\n",
    "print(f'Pi according to estimation is {pi}.')\n",
    "\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "ax.scatter(x_inside, y_inside, color='g', marker='s')\n",
    "ax.scatter(x_outside, y_outside, color='r', marker='s')\n",
    "fig.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning formulates interaction between an **agent** and its **environment** as **Markov decision processes**. For a given **state**, an agent takes an **action** based on the current **state**. In response to that action at that state, the agent will then get some **reward** from the environment, and that state changes to the next one.\n",
    "\n",
    "$S_0 \\rightarrow A_0 \\rightarrow R_1 \\rightarrow S_1 \\rightarrow A_1 \\rightarrow R_2 \\rightarrow S_2 \\rightarrow ... \\rightarrow S_{t-1} \\rightarrow A_{t-1} \\rightarrow R_t \\rightarrow S_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL Framework](img/rl_framework.png)\n",
    "Source: [Sutton and Barto](https://cdn.preterhuman.net/texts/science_and_technology/artificial_intelligence/Reinforcement%20Learning%20%20An%20Introduction%20-%20Richard%20S.%20Sutton%20,%20Andrew%20G.%20Barto.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This lab: How do we deal with stochastic environment and policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main approaches in solving reinforcement learning: **model-based** and **model-free** approaches. \n",
    "\n",
    "The model-based approach assumes that we have some or full knowledge of how our environment works (God-mode GridWorld).\n",
    "\n",
    "**The model-free approach relies on our agent to explore the environment without any prior knowledge.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/monte_carlo.jpg\" alt=\"Monte Carlo Casino\" width=75%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Blackjack-v1](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to play the variant of blackjack problem described in Example 5.1 in [Reinforcement Learning: An Introduction by Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html). The rules are almost the same as a typical casino poker except for:\n",
    "* There are only two actions for the player: hit or stay\n",
    "* Cards are dealt randomly with replacement from a standard 52-card deck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/blackjack_scores.PNG\" alt=\"Blackjack Scores\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('Blackjack-v1', render_mode=\"human\")\n",
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept Assignment** Using the reinforcement learning scheme we learned from last class, what would Blackjack-v1 look like?\n",
    "\n",
    "* Environment: Blackjack table\n",
    "* Agent: Player\n",
    "* States: ????\n",
    "* Actions: ????\n",
    "* Rewards:  ????\n",
    "* Episodes: ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space (State Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is a tuple with three elements:\n",
    "* The player's current score (0-31)\n",
    "* The dealer's showing card (1-11)\n",
    "* 1 if the player has a usable ace; 0 if the player does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0 - stick\n",
    "* 1 - hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* +1 for winning\n",
    "* +0 for tie\n",
    "* -1 for losing\n",
    "* +1.5 for winning with a natural Blackjack (21 starting hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'State: {env.reset()}')\n",
    "print(f'Result: {env.step(0)} Dealer has {sum(env.dealer)}') #next state, reward, done, info\n",
    "\n",
    "# Note: ech time when the  user sticks, the game ends. 0 action = stick (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play a couple of rounds with your own natural neural networks to get a hang of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = env.reset()\n",
    "print(f'Starting hands: You have {state[0]}. Dealer is showing {state[1]}')\n",
    "done = False\n",
    "while not done:\n",
    "    action = int(input('Choose action: '))\n",
    "    state, reward, done, info, _ = env.step(action)\n",
    "    print(f'After taking action {action}, You have {state[0]}. Usable ace: {state[2]}')\n",
    "    if done:\n",
    "        print(f'You have {state[0]}, Dealer has {sum(env.dealer)}, Reward: {reward}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function `run_single_episode`  simulates an agent playing at random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_episode(agent):\n",
    "    result = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.getAction(state)\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        result.append((state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return(result, sum(env.dealer)) #must return a list of tuples (state,action,reward,next_state,done)\n",
    "\n",
    "randomAgent = BJAgent(env)\n",
    "res = run_single_episode(randomAgent)\n",
    "for elem in res[0]:\n",
    "    print(elem)\n",
    "    print()\n",
    "print(\"Dealer sum: \" + str(res[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Strategy for Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Black Jack is the game with one of the best odds in the casino. [Ed Thorp (1966)](http://www.eecs.harvard.edu/cs286r/courses/fall12/papers/Thorpe_KellyCriterion2007.pdf) has derived the basic optimal strategy of playing the game using the [Kelly Criterion](http://www.herrold.com/brokerage/kelly.pdf) as the following figure. Those interested in the derivation should check out Thorp's book [Beat the Dealer](https://www.amazon.com/Beat-Dealer-Winning-Strategy-Twenty-One/dp/0394703103) as well as [Bringing Down The House](https://www.amazon.com/gp/product/0743249992/) by Ben Mezrich and its movie adaptation [21](https://www.imdb.com/title/tt0478087/). But for today, we will have nothing to do with the math and everything to do with reinforcement learning, Monte Carlo method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ed_thorp.PNG\" alt=\"Basic Optimal Strategy for Black Jack\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions runs an episode with a given policy.\n",
    "# The policy  is a dictionaryy mapping from state -> action\n",
    "# env.action_space.sample()  returns a random action  - you could use this one above !\n",
    "def run_single_episode_with_policy(policy=None):\n",
    "    result = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = None\n",
    "        if len(state) == 1:\n",
    "            action = policy[state] if policy else env.action_space.sample()\n",
    "        elif len(state) == 2:\n",
    "            action = policy[state[0]] if policy else env.action_space.sample()\n",
    "        elif len(state) == 3:\n",
    "            action = policy[state] if policy else env.action_space.sample()\n",
    "\n",
    "        \n",
    "        next_state,reward,done,info,_ = env.step(action)\n",
    "        result.append((state,action,reward,next_state,done))\n",
    "        state = next_state\n",
    "        if done: break\n",
    "    return(result) #must return a list of tuples (state,action,reward,next_state,done)\n",
    "\n",
    "#load saved optimal q dictionary, i.e. it maps state - > [Q(state, stick), Q(state, hit)]\n",
    "optimal_q = pickle.load(open(f'{MODEL_PATH}bj_optimal_q.pkl','rb'))\n",
    "\n",
    "#convert to optimal policy, i.e. a new dictionary mapping state - > argmax(two Q(state, *) actions)\n",
    "optimal_policy = {state:np.argmax(value) for state, value in optimal_q.items()}\n",
    "\n",
    "# Stick only policy\n",
    "stick_policy = {state: 0 for state,value in optimal_policy.items()}\n",
    "\n",
    "# Play a big number of episodes and compare the results between the following policies:\n",
    "# A. stick always\n",
    "# B. random policy\n",
    "# C. optimal policy\n",
    "n_trial = 100000\n",
    "stick_rewards = []\n",
    "random_rewards = []\n",
    "optimal_rewards = []\n",
    "for i in trange(n_trial): #  trange is from tdqm package to show some visual output with the iteration status\n",
    "    # Run episode under each policy\n",
    "    stick_res = run_single_episode_with_policy(stick_policy)\n",
    "    random_res = run_single_episode_with_policy()\n",
    "    optimal_res = run_single_episode_with_policy(optimal_policy)\n",
    "    \n",
    "    # -1 represents the last component index which gives us the final episode transitioon\n",
    "    # The reward is on position 2\n",
    "    stick_rewards.append(stick_res[-1][2])\n",
    "    random_rewards.append(random_res[-1][2])\n",
    "    optimal_rewards.append(optimal_res[-1][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a pandasdataframe (similar to a SQL table !)\n",
    "result = pd.DataFrame({'stick':stick_rewards,'random':random_rewards,'optimal':optimal_rewards})\n",
    "print(result[0:10]) # print the first 10 episode reward results as a demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot some stats about the database\n",
    "print(result.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make it more visible - put all variables in a column and the values in another one\n",
    "result_m = result.melt()\n",
    "print(result_m[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the melted dataset above, draw a barplot to compare visually between them\n",
    "sns.barplot(data=result_m, x ='variable', y ='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement on your own - Control Problem: Greedy within The Limit of Exploration (GLIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the first version of MC Control from the book (chapter 5.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1 studied:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the pseudocode for GLIE MC Control:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![First version from chapter 5.4](img/mc1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from course that we have a possibility to compute the sum incrementally:\n",
    "![Incrementa sum](img/mc2.PNG)\n",
    "\n",
    "\n",
    "Than we move to the GLIE method which performs the best and updates an e-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Greedy within The Limit of Exploration](img/mc_control_glie.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Assigment 1** Implement GLIE Monte Carlo control in file BJAgent.py `mc_control_glie`, then run the code below to create the agent in the environment, train the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MonteCarlo_BJAgent(env)\n",
    "agent.mc_control_glie(n_episode=50000, firstVisit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert from Q(s,a) to V(s) using the already implemented function, just run the things below to see the graphics output\n",
    "# Nothing else to implement\n",
    "agent.q_to_v()\n",
    "plot_blackjack_values(agent.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(agent.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the trained agent play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpisodes = 10\n",
    "\n",
    "agent = MonteCarlo_BJAgent(env)\n",
    "for epIndex in range(numEpisodes):\n",
    "    print(f\"***** Episode {epIndex} ******\")\n",
    "    transitions = run_single_episode(agent)\n",
    "    for elem in transitions[0]:\n",
    "        print(elem)\n",
    "    print(transitions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use MC to solve the other environments as well:\n",
    "    * FrozenLake-v1\n",
    "    * Taxi-v2\n",
    "    * Any other environments with discrete states and actions at [OpenAI Gym](https://github.com/openai/gym/wiki/Table-of-environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
